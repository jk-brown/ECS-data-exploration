---
title: "Fitting distribution to Sherwood et al. data"
author: "Joe Brown"
date: "2023-10-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal

Use `fitdistr()` to maximize the likelihood of distirbution parameters to fit a provided set of data. I am using this as a quick and easy way to propose distributions for Sherwood et al. 2020 lines of evidence for ECS assuming that ECS follows a `gamma` distribution. 

This method will reproduce likely distribution following a gamma distbution that will have a similar peak and spread as the distributions presented in Sherwood et al. 2020. 

The optimal `shape` and `rate` parameters for each `gamma` distribution of ECS for different lines of evidence will be used in `rlgamma()` to produce samples that can be propagated through Matilda/Hector. 

# Libraries

```{r}
# load libraries
library(MASS)
library(ggplot2)
remotes::install_github("jgcri/matilda@dev", force = T)
library(matilda)
library(parallel)
```

# Data

Adding data for potential ECS values from Sherwood PDFs using the values of percentiles (5th, 10th, 17th, 20th, 25th, 50th, 75th, 80th, 83rd, 90th, and 95th) mode and mean.

Storing data as a list of vectors with names of the vectors in the list coded to represent the line of evidence associated with that data.

```{r}
data_list <- list(
  UL = c(
    2.1950,
    2.3849,
    2.5750,
    2.6450,
    2.7450,
    3.2150,
    3.7850,
    3.9450,
    4.055,
    4.4149,
    4.8650,
    3.0249,
    3.33741
  ),
  no_process = c(
    2.0850,
    2.2850,
    2.4750,
    2.5450,
    2.6550,
    3.1649,
    3.8150,
    4.0050,
    4.1450,
    4.5750,
    5.1450,
    2.9149,
    3.341
  ),
  no_historical = c(
    2.0450,
    2.2250,
    2.4050,
    2.4750,
    2.5750,
    3.0450,
    3.6250,
    3.7850,
    3.9050,
    4.2749,
    4.7550,
    2.8150,
    3.1799
  ),
  no_paleo = c(
    2.0549,
    2.3150,
    2.5750,
    2.6649,
    2.8150,
    3.5249,
    4.5150,
    4.8250,
    5.0650,
    5.8650,
    7.0750,
    3.0050,
    3.9729
  ),
  EC_UL = c(
    2.2749,
    2.4750,
    2.6649,
    2.7350,
    2.8350,
    3.3150,
    3.8849,
    4.0450,
    4.1550,
    4.5150,
    4.9550,
    3.0650,
    3.4313
  )
)
```

# Fit Distirbution to the data 

Use the `fitdistr()` function in the `MASS` package to fit a maximum likelihood `gamma` distribution to each element in the data list. It is possible to manipulated in several ways. I will avoid that for now to minimize bias.

```{r, results='hide'}

hyper_param_list <- lapply(data_list, fitdistr, densfun = "gamma")

```

The parameter list includes estimated `shape` and `rate` parameters for each evidence PDF. Each element of the list also produces a log-likelihood value.

# Sample Distributions

Use the parameters for each line of evidence to simulate n samples from a gamma distribution.

```{r, results='hide'}
set.seed(2)

n = 10000

sample_list <- lapply(hyper_param_list, function(evidence) {
  data.frame(ECS = rgamma(n,
                          shape = evidence$estimate["shape"],
                          rate = evidence$estimate["rate"]))
})
  
```

# Visualize Simulated Samples

Use ggplot2 to visualize samples simulated from the gamma distribution for each line of evidence.

```{r, results='hide'}
# create dataframe
ecs_samples_df <- data.frame(
  value = unlist(sample_list),
  evidence = rep(names(sample_list), each = n),
  row.names = NULL
)

```

```{r}
# plot
ggplot() +
  geom_density(data = ecs_samples_df,
               aes(x = value, 
                   color = evidence)) +
  theme_light() +
  scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) +
  facet_wrap(~evidence)
  
```

# Using ECS samples to run Matilda

Samples from the distributions above can be used as the ECS inputs for Hector/Matilda.

Read in scenario inputs:
```{r}
ini_list <- list(
ssp126 = system.file("input/hector_ssp126.ini", package = "hector"), 
ssp245 = system.file("input/hector_ssp245.ini", package = "hector"), 
ssp370 = system.file("input/hector_ssp370.ini", package = "hector"),
ssp585 = system.file("input/hector_ssp585.ini", package = "hector") 
)
```

Initiate cores:
```{r}
core_list <- lapply(ini_list, newcore)
```

Generate parameter values using Matilda's priors. 

To isolate the effect of ECS values - use one core to produce all parameter values:
```{r}
set.seed(123)
init_params <- generate_params(core = core_list[[1]], draws = n)
```

For each parameter set in the list, remove the ECS column and add parameters to ECS samples from gamma sampling effort:
```{r}
params_list <- lapply(sample_list, function(data){
  
  params_no_ecs <- init_params
  
  params_no_ecs$ECS <- NULL
  
  cbind(data, params_no_ecs)
  
})

# just one evidence scenario
test_one_param_df <- list(params_list$UL)
```

Use parallel computing to run Hector for each of the parameter set in `params_list` over :

```{r}
cl <- makeCluster(detectCores() - 1)

clusterExport(cl, c("iterate_model", "params_list", "ini_list", "newcore"))

start <- proc.time()
result <- parLapply(cl, params_list, function(params) {
  core = newcore(ini_list$ssp245)
  
  iterate_model(
    core = core,
    params = params,
    save_years = 1800:2100,
    save_vars = c("CO2_concentration", "gmst")
  )
})

stopCluster(cl)
proc.time() - start
```

Running the above code chunk on 8 core cluster applies 10,000 parameter sets for each line of evidence in params_list across the SSP2-4.5 emissions scenario. The code results in a list of result dfs, one for each line of evidence. Each df will have a total of 6,020,000 rows (1 scenario x 2 variables x 301 years x 10000 runs).

Loop through the list of result dfs and add a column identifying its line of evidence. Do this by creating a new df based on the name of the df in the result_na_rm list, then add an evidence column to each new df that is the name of the df in result_na_rm, and last replace the old df with the new one in the result_na_rm list

```{r}
for(name in names(result)) {
  df <- result[[name]]
  df$evidence <- name
  result[[name]] <- df
}
```

# Weighting ensemble members against observed data

Weight models based on observed temperature using score_ramp (w1 = 0.0, w2 = sd(observed gmst))

```{r}
ramp_scored <- lapply(
  result,
  score_runs,
  criterion = criterion_gmst_obs(),
  score_function = score_ramp,
  w1 = 0.0,
  w2 = sd(matilda:::adjusted_gmst_data$anomaly_C)
)
```

Just to have it, weight models using the score_bayesian function.

```{r}
bayesian_scored <- lapply(result,
                          score_runs,
                          criterion = criterion_gmst_obs(),
                          score_function = score_bayesian)
```

Merge the weights with results:

```{r}
result_scored <- Map(merge, result, ramp_scored, by = "run_number")
```

and create a df (for plotting):
```{r}
result_scored_df <- do.call(rbind, result_scored)
row.names(result_scored_df) <- NULL
```

NAs may be present. These need to be removed before proceeding with analysis.

```{r}
anyNA(result_scored_df)
result_scored_df <- na.omit(result_scored_df)
```

# Calculate output metrics 

Define metric of interest. What is the long-term average (2090-2100) of gmst anomaly?

```{r}
long_term_metric <- new_metric(var = GMST(), years = 2090:2100, mean)
```

Apply metric to each df in `result_na_rm`:

```{r}
metric_results <- lapply(result_na_rm, metric_calc, long_term_metric)
```

# Calculate warming probabilities

Compute the probability of warming range for each evidence df. Need access to the metric values and weights.

Add scores to the `metric_results` list to make the probability calculation code easier. 

```{r}
metric_results_scored <- Map(merge, metric_results, ramp_scored, by = "run_number")
```

now calculate the probabilities using the `metric_results_scored` object

```{r}
bins <- c(1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, Inf)

prob_results <- lapply(metric_results_scored, function(df){
  
  prob_calc(df$metric_result, 
            bins = bins,
            scores = df$weights)
})
```

Add column of evidence labels

```{r}
for(name in names(prob_results)) {
  df <- prob_results[[name]]
  df$evidence <- name
  prob_results[[name]] <- df
}
```

create probability data frame for plotting - remove row names

```{r}
prob_df <- do.call(rbind, prob_results)
row.names(prob_df) <- NULL
```

# Some Viz

Probability plot:

```{r}
prob_plot <- 
  ggplot(data = prob_df, 
           aes(
             fill = bins, 
             x = evidence, 
             y = probability)) +
  geom_bar(position = position_fill(reverse = T),
           stat = "identity",
           width = 0.6)+
  scale_y_continuous(breaks = seq(0.0, 1.0, 0.1)) +
  scale_fill_manual(
     values = c(
      "#2166AC",
      "#4393C3",
      "#D1E5F0",
      "#FDDBC7",
      "#F4A582",
      "#D6604D",
      "#B2182B",
      "#67001F"),
     labels =  c(
      "1.0 to 1.5 C",
      "1.5 to 2.0 C",
      "2.0 to 2.5 C", # used this bin specifically for LTE seminar example
      "2.5 to 3.0 C",
      "3.0 to 3.5 C",
      "3.5 to 4 C",
      "4 to 4.5 C",
      ">4.5 C")) +
  coord_flip() +
  theme_light()
prob_plot
  
```
Projection plot:

to make this figure each of the result dfs need to be merged with scores.
For correct relative anomaly values (e.g., to pre-industrial) values need normalized (skipping this for right now).

```{r}
projection_plot <- 
  ggplot(data = subset(result_scored_df,
                       year >= 1950 
                       & year <= 2100
                       & variable == GMST())) +
  geom_line(
    aes(
      x = year, 
      y = value,
      group = run_number,
      color = weights,
      alpha = weights),
    linewidth = 0.3) +
  scale_color_gradient(low = "lightblue", high = "dodgerblue4", name = "Weights") +
  scale_alpha_continuous(range(c(0, 1))) +
  labs(x = "year", y = "temp") +
  theme_light() +
  guides(alpha = "none") +
  facet_wrap(~evidence)
projection_plot                           
```

```{r, message=FALSE}
# calculating median warming
library(tidyverse)
library(spatstat)

temp_data <- subset(result_scored_df, 
                    variable == GMST() &
                      year > 2015 &
                      year < 2101)

# median warming calculation
median_warming <- temp_data %>% 
  group_by(year, evidence) %>% 
  reframe(median_warming_wt = weighted.quantile(value, weights, probs = 0.5),
          CI_5 = weighted.quantile(value, weights, probs = 0.05),
          CI_95 = weighted.quantile(value, weights, probs = 0.95),
          CI_16 = weighted.quantile(value, weights, probs = 0.16),
          CI_84 = weighted.quantile(value, weights, probs = 0.84))

median_warming
```

```{r}
temp_plot <- 
  ggplot(data = median_warming) +
  geom_line(aes(x = year, 
                y = median_warming_wt,
                color = evidence)) +
  geom_ribbon(aes(x = year, 
                  ymin = CI_5,
                  ymax = CI_95,
                  fill = evidence),
              alpha = 0.1) +
  theme_light() +
  facet_wrap(~evidence)
temp_plot
  
```
```{r}
# median warming calculation
warming_point <- temp_data %>% 
  group_by(evidence) %>% 
  reframe(median_warming_wt = weighted.quantile(value, weights, probs = 0.5),
          CI_5 = weighted.quantile(value, weights, probs = 0.05),
          CI_95 = weighted.quantile(value, weights, probs = 0.95),
          CI_16 = weighted.quantile(value, weights, probs = 0.16),
          CI_84 = weighted.quantile(value, weights, probs = 0.84))

warming_point
```
```{r}
point_plot <- 
  ggplot(data = warming_point) +
  geom_point(aes(x = evidence,
                 y = median_warming_wt,
                 color = evidence),
             size = 5,
             shape = 1) +
  geom_errorbar(aes(x = evidence,
                    ymin = CI_5,
                    ymax = CI_95,
                    color = evidence),
                width = 0.1,
                linewidth = 0.7) +
  theme_light()
point_plot
```


# Other notes etc.

#### *This has already been run - do not run again unless you need to save a new file.* 
Bind the rows of the dfs together to make a final large df with all results and save.

```{r}
result_df <- do.call(rbind, result_na_rm)
write.csv(result_df, file = "data/result_df_10k_na_rm.csv")
```




```{r}
y <- data.frame(
  value = rtrunc(100000, spec = "gamma", a = 1.0, b = 8.0, shape = 17.56, rate = 5.29))
```

```{r}
ggplot() +
  geom_density(data = y,
               aes(x = value)) +
  theme_light()
```

