---
title: "Fitting distribution to Sherwood et al. data"
author: "Joe Brown"
date: "2023-10-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Summary if this analysis

This document works through the analysis conducted to investigate the affect of using variable ECS evidence combinations to compute future temperature projections using a probabilistic simple climate model framework (Hector with Matilda).

Sherwood et al. 2020 (S20) used a sophisticated Bayesian approach to quantify a likelihood distribution using combined lines of ECS evidence (they called this their *Baseline*). This distribution has since been used by IPCC to update the likely ECS range. In S20, authors also systematically remove individual lines of evidence, use different priors, and add lines of evidence to investigate the impact these different configurations impact the ECS distributions.  
Here the goal was to take advantage of these ECS likelihood distributions, which have slight differences in peak and spread, and use them to project future climate using Hector with Matilda (simple climate model with a probabilistic projection framework). This will provide some perspective on the affect different ECS distributions have on global temperature projections and the probability of surpassing warming thresholds. To do this, data from confidence levels were harvested from S20 supplemental data and used to reproduce likelihood distributions for five evidence combinations highlighted in S20, these included:

1. *Baseline* - full combination of all lines of evidence. 

2. *No process* - excluding process and feedback evidence (from ESMs) from the full baseline combination.

3. *No historical* - excluding historical evidence from the full baseline combination. 

4. *No paleoclimate* - excluding paleoclimate evidence from the full baseline combination.

5. *Baseline + Emergent constraints* - using the baseline combination with the inclusion of evidence from emergent constraints. 

A parametric likelihood distribution was produced by optimizing the shape and rate parameters of a gamma distribution using the S20 data for each of the evidence configurations. The purpose of this method was to reproduce distributions with similar peak and spread as the official likelihood distributions quantified in S20, that can also be sampled with little effort. Using this method, ECS values can be sampled n times (while taking into account estimated uncertainty) from distributions that mimic those developed in S20. This was completed for each evidence configuration.

Sampled ECS values for each evidence configuration was used to establish a parameter set that was used to complete a single run of the Hector simple climate model. Using the Matilda probabilistic framework this process is iterated over several parameter sets, each with a slightly different ECS value to produce a perturbed parameter ensemble (PPE) result. As a result, the uncertainty associated with ECS from different evidence combinations is propagated through an ensemble of modeled climate projections. 

PPEs are weighted against historical data (specifically historical temperature and CO2 concentrations). This process down weights ensemble members that significantly diverge from observed values while assigning higher weights to ensemble members that perform well during the historical period. This step is performed under the assumption that ensembles members that perform well in the historical period have a higher potential to more accurately projected future climate variables.

Using assigned weights, end-of-century metrics and probabilities of warming were calculated from the ensemble (i.e., median global mean surface (gmst) for 2100). Probabilities of warming are a weighted sum of median temperature anomaly values that fall within a specific warming range.

This document offers code for completing the above analysis and also for creating figures presented at for on poster #GC33F-1213 at AGU 2023.

*Note:* Running this analysis may take more that 7 hours given the mass of model runs used to stabilize resulting temperature distributions for each of the evidence configurations (n = 10,000 x 5 evidence scenarios = 50,000 total model runs). This may also take up considerable memory. Efforts were taken to provide efficient and easy to understand code. However, some memory inefficiencies do currently exist.

# Libraries

```{r, message=FALSE, results='hide'}
# load libraries
library(MASS)
library(ggplot2)
library(see)
# force load the dev branch of Matilda to pick-up error corrections
# if on windows and R won't install first run this in console --> options(download.file.method = "wininet") 
remotes::install_github("jgcri/matilda@dev", force = T)
library(matilda)
library(parallel)
```

# Data

Here ECS data sets are built for each evidence configuration using ECS values from S20. This data was harvested from supplemental information in S20 and represents ECS percentile estimates from likelihood distributions quantified in S20 for each of the five evidence configurations (other configurations are available in S20 beyond these five). The percentile estimates reported in this data represent the 5th, 10th, 17th, 20th, 25th, 50th, 75th, 80th, 83rd, 90th, and 95th percentiles as well as the mode and mean.

Store data as a list of vectors, this makes programming the rest of the analysis easier. Names of each list are coded to represent the evidence configuration each data set in the data list is associated with.

```{r}
data_list <- list(
  "Baseline" = c(
    2.1950,
    2.3849,
    2.5750,
    2.6450,
    2.7450,
    3.2150,
    3.7850,
    3.9450,
    4.055,
    4.4149,
    4.8650,
    3.0249,
    3.33741
  ),
  "No Process" = c(
    2.0850,
    2.2850,
    2.4750,
    2.5450,
    2.6550,
    3.1649,
    3.8150,
    4.0050,
    4.1450,
    4.5750,
    5.1450,
    2.9149,
    3.341
  ),
  "No Historical" = c(
    2.0450,
    2.2250,
    2.4050,
    2.4750,
    2.5750,
    3.0450,
    3.6250,
    3.7850,
    3.9050,
    4.2749,
    4.7550,
    2.8150,
    3.1799
  ),
  "No Paleoclimate" = c(
    2.0549,
    2.3150,
    2.5750,
    2.6649,
    2.8150,
    3.5249,
    4.5150,
    4.8250,
    5.0650,
    5.8650,
    7.0750,
    3.0050,
    3.9729
  ),
  "Baseline + Emergent constraints" = c(
    2.2749,
    2.4750,
    2.6649,
    2.7350,
    2.8350,
    3.3150,
    3.8849,
    4.0450,
    4.1550,
    4.5150,
    4.9550,
    3.0650,
    3.4313
  )
)
```

# Fit Distirbution to the data 

Here I am using `fitdistr()` (function in the `MASS` package) to maximize the likelihood of distribution parameters to fit a provided set of data. I am using this as a quick and easy way to propose/reproduce distributions for S20 evidence configurations, assuming that ECS follows a gamma distribution. 

This method reproduces the likelihood distribution that has similar peak and spread as the distributions presented in S20 and is easily sampled. The result of `fitdistr()` is the optimal `shape` and `rate` parameters of the gamma distributions for each evidence configuration. These distribution parameters will be used in `rlgamma()` to produce samples that characterize the uncertainty of ECS from different evidence configurations. This error can be propagated through Hector/Matilda. 

Brief notes on `fitdistr()`: This function fits a maximum likelihood distribution to each element in the data list. It is possible to manipulate it in several ways, but I don't make any of these changes to minimize bias on the resulting distribution.

```{r, results='hide'}
# Produce the rate and shape parameters for the gamma distributions for each 
# element in `data_list`.
# 
# `lapply()` will run `fitdistr()` with the gamma density function (`densfun`) 
# for each element in `data_list`

hyper_param_list <- lapply(data_list, 
                           fitdistr, 
                           densfun = "gamma")

```

The result is a new list (`hyper_param_list`) of estimated `rate` and `shape` parameters of a gamma distribution for each evidence configuration.

# Sample Distributions

Using the gamma distribution parameters produced in the previous code chunk, I want to obtain a sample of ECS values for each evidence configuration. 

Here I produce n samples for each of the evidence configurations using `rgamma` with the `shape` and `rate` parameters stored in `hyper_param_list`:

```{r, results='hide'}
# set seed for reproducible results
set.seed(2)

# set n value - how many ECS samples produced for each evidence configuration
n = 10000

# Generate a list of data frames containing ECS samples.
# For each evidence configuration using gamma parameters stored in 
# 'hyper_param_list'.
# 
# The 'lapply' function applies a function to each element in 'hyper_param_list'
# to construct a data frame of ECS samples using 'rgamma'.
# 
# The shape and rate parameters are retrieved from each element 
# (referred to as 'evidence').
sample_list <- lapply(hyper_param_list, 
                      function(evidence) {
                        
                        data.frame(ECS = rgamma(n,
                                                shape = evidence$estimate["shape"],
                                                rate = evidence$estimate["rate"]))
                        
                        })
```

The result is a new list (`sample_list`) of data frames, one for each evidence scenario, that contains a vector of ECS values sampled from respective gamma distributions. 

# Visualize Simulated Samples

Once the ECS samples are produced and stored in a list of data frames (`sample_list`), the sample distribution can be visualized with `ggplot2`.

Before working in `ggplot2`, I build a `sample_list` to a single data frame because it is easier to plot. 

```{r, results='hide'}
# Create a new data frame 'ecs_samples_df' from elements in 'sample_list'.
# 
#   'value': Unlist ECS samples from 'sample_list'.
#            This operation combines the vectors of each 
#            element in 'sample_list' into a single vector under 
#            the 'value' column.
#            
#  'evidence': Repeat names of each element in 'sample_list' 'n' times.
#     Note: 'n' is set to 10,000 for each evidence combination.
#   
# Last, Remove row names to ensure a clean result.
ecs_samples_df <- data.frame(
  
  value = unlist(sample_list),
  evidence = rep(names(sample_list), each = n),
  row.names = NULL
  
)
```

The result is a data frame that combines ECS values sampled from gamma distributions for each evidence configuration (10,000 samples x 5 evidence scenarios = 50,000 rows). Each row is labeled with the name of the evidence configuration the value belongs to.   

Plot ECS samples as a`geom_density` with faceted by 'evidence':

```{r}
# Plot sample distributions of ECS for each evidence configuration.
# 
# Use `geom_density()` plot with desired scale (`scale_x_continuous`).
# Facet figure by `evidence` (evidence configuration name). 
ggplot() +
  geom_density(data = ecs_samples_df,
               aes(x = value, 
                   color = evidence)) +
  theme_light() +
  scale_x_continuous(breaks = seq(from = 0, to = 10, 
                                  by = 1)) +
  facet_wrap(~ evidence)
  
# Save the figure!
ggsave("figures/ECS_distributions.png",
       device = "png",
       width = 10,
       height = 8,
       units = "in",
       dpi = 300)
```

# Using ECS samples to run Matilda

Samples from the distributions above can be used as the ECS inputs for Hector/Matilda. This will propagate uncertainty of ECS, estimated using the gamma distribution samples, for each of the evidence configurations.

Hector/Matilda need an emissions scenario to run. Loading these scenarios into R from ini files is explained in detail in the Hector beginner tutorial.

For this analysis I use the SSP 2-4.5 scenario. This analysis can be completed across a number of different SSP scenarios that are stored in the Hector package. I create a list of commonly used scenarios (`ini_list`), but only use one scenario (SSP2-4.5) for the rest of the analysis. This is not necessarily efficient here, but if I want to run the analysis on a new scenario I want it to be loaded and stored in the list. This makes re-running the analysis with a new scenario easier.

```{r, results='hide'}
# Read in scenario ini files and store them in a list
ini_list <- list(
ssp126 = system.file("input/hector_ssp126.ini", package = "hector"), 
ssp245 = system.file("input/hector_ssp245.ini", package = "hector"), 
ssp370 = system.file("input/hector_ssp370.ini", package = "hector"),
ssp585 = system.file("input/hector_ssp585.ini", package = "hector") 
)
```

The ini files are used to initiate `core` environments in Hector (more details in Hector beginnger tutorial). 

Here, I use `lapply()` to initiate `cores` (using `newcore()`) for all the scenarios loaded above and store in a core_list. Again this is not very efficient since I am only using one scenario for the current analysis. But makes re-running with a new scenario easier.

```{r, results='hide'}
# Initiate model core for each element of in ini_list
# 
# `lapply()` will run `newcore` for each element in `ini_list` 
core_list <- lapply(ini_list, newcore)
```

The result of will be a new list (`core_list`) where each element in the list is a `core` that can be used to run a Hector.

# Generate parameter values using Matilda's priors 

Matilda runs under the assumption that multiple perturbed parameter sets are being used to run Hector over multiple iterations to produce an ensemble of model outputs. Parameter sets are produced in Matilda using `generate_params()`.

In this analysis I use `generate_params` to produce and initial parameter set (`init_params`). This will produce `n` sets of parameters that will run apply parameter uncertainty to Hector runs. There is and ECS (`ECS`) parameter column produced from this step. However, these ECS values are produced from the priors programmed into Matilda's parameter sampling procedure, not from the evidence configuration gamma distributions this code is analyzing. Therefore, this column will need to be replaced in a separate step. 

I generate a `n` sets of model parameters using the one `core` (the first element in `core_list`):
```{r, results='hide'}
# Set seed for reproducible result
set.seed(123)

# Use first element in core_list to generate n number of draws for each 
# parameter. Store in a new object.
init_params <- generate_params(core = core_list[[1]], draws = n)
```

The result will be a new data frame object (`init_params`) with 10,000 samples for 6 columns labeled for the variables they represent. 

*Note:* **Again** it is important to note that this data frame has a column for `ECS` - but that these values are sample from a prior distribution...not the distributions chosen for the analysis at hand.

For each parameter set in the list, remove current ECS column and replace with ECS samples from gamma sampling effort:
```{r}
# Creating a list of modified data frames based on ECS values in `sample_list`
params_list <- lapply(sample_list, function(data){
  
  # copying the initial parameter values to a new object
  params_no_ecs <- init_params
  
  # Removing the 'ECS' column from the copied parameter data frame
  params_no_ecs$ECS <- NULL
  
  # Combing the current data frame (from sample_list) with the modified 
  # parameter data frame (param_no_ecs)
  cbind(data, params_no_ecs)
  
})

# Extracting a specific data frame from `params_list` as a check
# Here, extracting the 'Baseline' data frame to assess if ECS column was 
# replaced correctly
test_one_param_df <- list(params_list$Baseline)
```

# Run Hector with Matilda

With the full list of parameter data frames now in `params_list` for each of the evidence configurations, I use them to run Hector with Matilda to conduct a single Hector run for each of the 10,000 parameter sets (per each evidence configuration = 50,000 total runs). 

To make this process more efficient, I use parallel computing to spread the job over several threads on my local CPU.

Use parallel computing to run Hector/Matilda for each of the parameter set in `params_list` using the SSP2-4.5 scenario:
```{r}
# Create a "cluster" of parallel workers, each corresponding to a CPU core.
# Here I use all cores except for one.
cl <- makeCluster(detectCores() - 1)

# Export the necessary functions and data to the parallel cluster.
# This should include any functions or data that are referenced when running 
# the paralleled loop.
clusterExport(cl, c("iterate_model", "params_list", "ini_list", "newcore"))

# Record the start time as a performance measurement
start <- proc.time()

# Use simple parallel computing to: 
# 
#   1) Initialize a core using the ini file for SSP2-4.5 -- a new core will be
#      initialized for each element of `params_list`
#   
#   2) Apply `iterate_model` to each element of 
#      `params_list`
#      
result <- parLapply(cl, params_list, function(params) {
  
  # 1) Initialize SSP2-4.5 scenario core
  core = newcore(ini_list$ssp245)
  
  # 2) Call iterate_model for current element in `params_list`
  iterate_model(
    core = core,
    params = params,
    save_years = 1800:2100, # identifies years to save
    save_vars = c("CO2_concentration", "gmst") # identifies output variables to save
  )
})

# Stop the parallel "cluster" after computation is complete
stopCluster(cl)

# Calculate and print the elapsed time for performance analysis
proc.time() - start
```

This code results in a list of Matilda result data frames, one for each evidence configuration. Each data frame will have a total of 6,020,000 rows (1 scenario x 2 variables x 301 years x 10000 runs).

Running this number of iterations may have resulted in some failed runs. The failed runs are identified by NAs in the result data frame for each evidence configuration in `result`. It is crucial to note that before NAs are removed, model ensemble members should be weighted. It's important to weight ensemble members BEFORE removing NAs in the result data frames. This is a programming detail that we are currently working to improve.

Before weighting the ensemble members in the data frames, I complete some post-processing. I loop through the list of result data frames (for each evidence configuration) and add a column identifying its evidence configuration. This additional step modifies each data frame element in `result` that will make future steps easier to complete and interpret.

```{r}
# Post-processing step before weighting ensemble members.

# Loop through elements in the list of `result` data frames (evidence configurations).
for(name in names(result)) {
  
  # Retrieve the current data frame associated with the evidence configuration. 
  df <- result[[name]]
  
  # Add a new column named "evidence" to the data frame and populate it with
  # the name of the current element (evidence configuration)
  df$evidence <- name
  
  # Update the original data frame in `result` list with the modified data frame 
  # that includes the "evidence column. 
  result[[name]] <- df
}
```

The result of this section of code is a list of results for each evidence configuration (`result`). In the `result` object, each element is a Matilda result, one for each of the evidence configurations being considered here. 

With these results, I can determine how well individual ensemble members (for each evidence configuration) represents observed historical data. This is done in the "scoring" or "weighting" step and will down weight ensemble members that don't accurately represent observed values, under the assumption that ensembles that are skilled at representing historical data are also skilled at projecting futures.

# Weighting ensemble members against observed data

In this analysis I weight ensemble members using two methods:

1) Historical temperature

2) Historical atmospheric CO2 concentrations 

Details for each of these weighting criteria can be found in the Matilda vignette and/or in the Matilda v1.0 software description pre-print.

There are two basic methods available for weighting in Matilda. Here, I use the `score_ramp` method. In this method I identify lower and upper bounds. As average ensemble difference diverges, weights degrade until surpassing an upper divergence limit (standard deviation of the observed data), at which point ensemble members are culled. 

Complete both individual scoring methods (temperature and CO2) then combine them to produce a multi-criteria weight. This is completed for each element of the `result` list: 
```{r}
# For each element (df) in `result`:
# 
#   - score using observed temperature (GMST)
#   
#   - score using observed CO2
#   
#   - omit NAs in both cases
#   
#   - combine scores with `multi_criteria_weighting`
scored_result <- lapply(result, function(df) {
  
  # Score ensemble members based on temperature (GMST)
  scores_gmst = score_runs(df,
                           criterion = criterion_gmst_obs(),
                           score_function = score_ramp, # can change scoring function here
                           w1 = 0.0, # adjust lower divergence bound
                           w2 = sd(matilda:::adjusted_gmst_data$anomaly_C)) # adjust upper divergence bound
  
  # Omit NAs from temperature scored result -- IMPORTANT
  scores_gmst <- na.omit(scores_gmst)
  
  # Score ensemble members based on CO2 concentration 
  scores_co2 = score_runs(df,
                          criterion = criterion_co2_obs(),
                          score_function = score_ramp, # can change scoring function here
                          w1 = 0.0, # adjust lower divergence bound
                          w2 = sd(matilda:::observed_data_co2$co2_ppm)) # adjust upper divergence bound
  
  # Omit NAs from CO2 scored result -- IMPORTANT 
  scores_co2 <- na.omit(scores_co2)
  
  # Combine individual scores into a list
  score_list = list(scores_co2, scores_gmst)
  
  # Calculate multi-criteria weighting with `score_list`
  scores_mc = multi_criteria_weighting(score_list)
  
  # Merge scores with the original data frame based on run_number
  # This will add rows for each scoring method
  scored_result = merge(df, scores_gmst, by = "run_number")
  scored_result = merge(scored_result, scores_co2, by = "run_number")
  scored_result = merge(scored_result, scores_mc, by = "run_number")
    
})

# Combine the scored results into a data frame
result_scored <- do.call(rbind, scored_result)

# Reset row names to avoid potential issues
row.names(result_scored) <- NULL
```

The result of this code will be a large data frame object that contains Matilda results for each evidence configuration with 3 new columns, one for each scoring method. There should no longer be NAs in the data frame. Any NAs that existed after producing list of Matilda `result` data frames for each evidence configuration should have been omitted in this code chunk. As a fail safe or checking step, run `anyNA()` on the `result_scored` data frame.

normalizing function:

```{r}
# Write function to normalize Matilda data to reference period
normalize_temperature <- function(data, reference_start_year, reference_end_year) {
  # Filter data for the reference period
  reference_period <- subset(
    data,
    year >= reference_start_year &
      year <= reference_end_year
  )

  # Calculate the mean values of reference period
  mean_reference_period <- mean(reference_period$value)

  # Calculate normalized values for each year in the data set
  ## subtract data values by reference period mean
  normalized_values <- data$value - mean_reference_period

  # Create a new data frame with the normalized data
  normalized_data <- data.frame(
    year = data$year,
    adjusted_value = normalized_values
  )

  return(normalized_data)
}
```

Now subset data to variable we want to plot. Trying to normalize with both CO2 and gmst in the df will cause the adjusted values to be miscalculated.
```{r, message=FALSE}
# calculating median warming
library(tidyverse)
library(spatstat)

temp_data <- subset(result_scored, 
                    variable == GMST() &
                      year > 1849 &
                      year < 2101)
```

normalizing temperature projections in temp_data df:

```{r}
# Normalize Matilda result to 1850-1900 reference period
pre_temp_data <- normalize_temperature(temp_data,
  reference_start_year = 1850,
  reference_end_year = 1900
)

# Add column of adjusted values
temp_data$value_adjusted <- pre_temp_data$adjusted_value

# remove the pre_result df to save memory
rm(pre_temp_data)
```

Plotting:
```{r}
gmst_scored_ensemble <- 
  ggplot(data = subset(temp_data,
                       year >= 1950 
                       & year <= 2100
                       & variable == GMST()
                       & evidence == "UL")) +
  geom_line(
    aes(
      x = year, 
      y = value,
      group = run_number,
      color = weights.x,
      alpha = weights.x),
    linewidth = 0.1) +
  scale_color_gradient(low = "lightblue", high = "dodgerblue4", name = "Weights") +
  scale_alpha_continuous(range(c(0, 1))) +
  labs(x = "Years", y = "Temperature Anomaly (C)") +
  ggtitle(label = "Hector PPE weighted by historical temperature") +
  theme_light() +
  guides(alpha = "none")

# Creates observed data frame - this  can be added as a layer to the plot
# But currently only includes data from 1950-2023
obs_dat <- data.frame(
  year = criterion_gmst_obs()$year,
  value_obs = criterion_gmst_obs()$obs_values
)

# Add observed CO2 values to aid visualization of most plausible models
gmst_scored_ensemble_obs <- gmst_scored_ensemble + 
  geom_line(
  data = obs_dat, aes(x = year, y = value_obs),
  color = "red",
  linewidth = 1
)
gmst_scored_ensemble_obs

ggsave("figures/gmst_scored_ensemble.png",
       plot = gmst_scored_ensemble_obs,
       device = "png",
       width = 28,
       height = 15, 
       units = "cm",
       dpi = "print")
```
### ensembles weighted with CO2 data

Now subset data to variable we want to plot. Trying to normalize with both CO2 and gmst in the df will cause the adjusted values to be miscalculated.
```{r, message=FALSE}
# calculating median warming
library(tidyverse)
library(spatstat)

co2_data <- subset(result_scored, 
                    variable == CONCENTRATIONS_CO2() &
                    year > 1849 &
                    year < 2101)
```

normalizing temperature projections in temp_data df:

```{r}
# Normalize Matilda result to 1850-1900 reference period
pre_co2_data <- normalize_temperature(co2_data,
  reference_start_year = 1850,
  reference_end_year = 1900
)

# Add column of adjusted values
co2_data$value_adjusted <- pre_co2_data$adjusted_value

# remove the pre_result df to save memory
rm(pre_co2_data)
```

Plotting:
```{r}
co2_scored_ensemble <- 
  ggplot(data = subset(co2_data,
                       year >= 1950 
                       & year <= 2100
                       & variable == CONCENTRATIONS_CO2()
                       & evidence == "UL")) +
  geom_line(
    aes(
      x = year, 
      y = value,
      group = run_number,
      color = weights.y,
      alpha = weights.y),
    linewidth = 0.1) +
  scale_color_gradient(low = "lightblue", high = "dodgerblue4", name = "Weights") +
  scale_alpha_continuous(range(c(0, 1))) +
  labs(x = "Years", y = "Atmospheric CO2 Concentration (ppm)") +
  ggtitle(label = "Hector PPE weighted by historical CO2 concentration") +
  theme_light() +
  guides(alpha = "none")

# Creates observed data frame - this  can be added as a layer to the plot
# But currently only includes data from 1950-2023
obs_dat <- data.frame(
  year = criterion_co2_obs()$year,
  value_obs = criterion_co2_obs()$obs_values
)

# Add observed CO2 values to aid visualization of most plausible models
co2_scored_ensemble_obs <- co2_scored_ensemble + 
  geom_line(
  data = obs_dat, aes(x = year, y = value_obs),
  color = "red",
  linewidth = 1
)
co2_scored_ensemble_obs

ggsave("figures/co2_scored_ensemble.png",
       plot = co2_scored_ensemble_obs,
       device = "png",
       width = 28,
       height = 15, 
       units = "cm",
       dpi = "print")
```

## multiple criterion weighting

```{r}
scored_mc <- lapply(result, function(df) {
  
  scores_co2_na = score_runs(df, 
                          criterion = criterion_co2_obs(),
                          score_function = score_ramp, # can change scoring function here
                          w1 = 0.0, # adjust lower divergence bound
                          w2 = sd(matilda:::observed_data_co2$co2_ppm)
                          ) # adjust upper divergence bound
  scores_co2 <- na.omit(scores_co2_na)
  
  scores_gmst_na = score_runs(df,
                           criterion = criterion_gmst_obs(),
                           score_function = score_ramp, # can change scoring function here
                           w1 = 0.0, # adjust lower divergence bound
                           w2 = sd(matilda:::adjusted_gmst_data$anomaly_C)
                           ) # adjust upper divergence bound
  scores_gmst <- na.omit(scores_gmst_na)
  
  score_list = list(scores_co2, scores_gmst)
  
  scores = multi_criteria_weighting(score_list)
    
})

# result scored mc is a list -- use lapply to merge and then clean
result_mc_scored_df <- lapply(scored_mc, function(df){
  
  result_scored_mc = merge(df, result_scored, by = "run_number")

  })

result_scored_mc_df <- do.call(rbind, result_scored_mc)

row.names(result_scored_mc_df) <- NULL
anyNA(result_scored_mc_df)
```

# Calculate output metrics 

Define metric of interest. What is the long-term average (2090-2100) of gmst anomaly?

```{r}
long_term_metric <- new_metric(var = GMST(), years = 2100, median)
```

Calculate metric for each df in `result` after removing NAs:

```{r}
metric_results <- lapply(result, function(df) {
  
  result_na_rm <- na.omit(df)
  
  metric_calc(result_na_rm, long_term_metric)
  
})
```

# Calculate warming probabilities

Compute the probability of warming range for each evidence df. Need access to the metric values and weights.

Add scores to the `metric_results` list to make the probability calculation code easier. 

```{r}
metric_results_scored <- Map(merge, metric_results, scored_mc, by = "run_number")


for(name in names(metric_results_scored)) {
  df <- metric_results_scored[[name]]
  df$evidence <- name
  metric_results_scored[[name]] <- df
}

# median warming calculation
metric_df <- do.call(rbind, metric_results_scored)
row.names(metric_df) <- NULL

```


now calculate the probabilities using the `metric_results_scored` object

```{r}
bins <- c(1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, Inf)

prob_results <- lapply(metric_results_scored, function(df){
  
  prob_calc(df$metric_result, 
            bins = bins,
            scores = df$mc_weight)
})
```

Add column of evidence labels

```{r}
for(name in names(prob_results)) {
  df <- prob_results[[name]]
  df$evidence <- name
  prob_results[[name]] <- df
}
```

create probability data frame for plotting - remove row names

```{r}
prob_df <- do.call(rbind, prob_results)
row.names(prob_df) <- NULL
prob_df$evidence <- as.factor(prob_df$evidence)
```

# Some Viz

Probability plot:

```{r}
prob_df$evidence <- recode_factor(prob_df$evidence,
                                  UL = "Baseline",
                                  no_process = "No Process",
                                  no_historical = "No Historical",
                                  no_paleo = "No Paleoclimate",
                                  EC_UL = "Baseline + Emergent constraints")

evidence_order <- c("Baseline", 
                    "No Process",
                    "No Historical",
                    "No Paleoclimate",
                    "Baseline + Emergent constraints")

prob_plot <- 
  ggplot(data = prob_df, 
           aes(
             fill = bins, 
             x = evidence, 
             y = probability)) +
  geom_bar(position = position_fill(reverse = T),
           stat = "identity",
           width = 0.6)+
  scale_y_continuous(breaks = seq(0.0, 1.0, 0.1)) +
  scale_fill_manual(
     values = c(
      "#2166AC",
      "#4393C3",
      "#D1E5F0",
      "#FDDBC7",
      "#F4A582",
      "#D6604D",
      "#B2182B",
      "#67001F"),
     labels =  c(
      expression(paste("1.0 to 1.5", ~degree, "C")),
      expression(paste("1.5 to 2.0", ~degree, "C")),
      expression(paste("2.0 to 2.5", ~degree, "C")), # used this bin specifically for LTE seminar example
      expression(paste("2.5 to 3.0", ~degree, "C")),
      expression(paste("3.0 to 3.5", ~degree, "C")),
      expression(paste("3.5 to 4.0", ~degree, "C")),
      expression(paste("4.0 to 4.5", ~degree, "C")),
      expression(paste(" > 4.5", ~degree, "C"))),
     name = "Warming") +
  labs(y = "Probability",
       x = NULL,
       title = "C)") +
  scale_x_discrete(limit = rev(evidence_order)) +
  theme_light(base_size = 24) +
  theme(legend.position ="bottom") +
  coord_flip()
prob_plot

ggsave("figures/temp_probabilities.png",
       plot = prob_plot,
       device = "png",
       width = 17.26,
       height = 11.39, 
       units = "in",
       dpi = "print")
  

```
*Projection plot:*
*to make this figure each of the result dfs need to be merged with scores.*
*For correct relative anomaly values (e.g., to pre-industrial) values need normalized. I need to think about this point more because it might not be accurate...*
*Normalization needs to be completed on CO2 and temperature separately in order to get an accurate results -- at least true for the function I have.*

Before computing median and confidence intervals, normalize the data to reference anomalies relative to pre-industrial temperature

Normalizing function:
Normalizing to plot pre-industrial reference:


Now subset data to variable we want to plot. Trying to normalize with both CO2 and gmst in the df will cause the adjusted values to be miscalculated.
```{r, message=FALSE}
# calculating median warming
library(tidyverse)
library(spatstat)

temp_data <- subset(result_scored, 
                    variable == GMST() &
                      year < 2101)
```

normalizing temperature projections in temp_data df:

```{r}
# Normalize Matilda result to 1850-1900 reference period
pre_temp_data <- normalize_temperature(temp_data,
  reference_start_year = 1850,
  reference_end_year = 1900
)

# Add column of adjusted values
temp_data$value_adjusted <- pre_temp_data$adjusted_value

# remove the pre_result df to save memory
rm(pre_temp_data)
```


```{r}
# median warming calculation
median_warming <- temp_data %>%
  group_by(year, evidence) %>%
  reframe(
    median_warming_wt = weighted.quantile(value_adjusted, mc_weight, probs = 0.5),
    CI_5 = weighted.quantile(value_adjusted, mc_weight, probs = 0.05),
    CI_95 = weighted.quantile(value_adjusted, mc_weight, probs = 0.95),
    CI_16 = weighted.quantile(value_adjusted, mc_weight, probs = 0.16),
    CI_84 = weighted.quantile(value_adjusted, mc_weight, probs = 0.84))

median_warming$evidence <- recode_factor(
  median_warming$evidence,
  UL = "Baseline",
  no_process = "No Process",
  no_historical = "No Historical",
  no_paleo = "No Paleoclimate",
  EC_UL = "Baseline + Emergent constraints")

median_warming
```

```{r}
CreateAllFacet <- function(df, col){
  df$facet <- df[[col]]
  temp <- df
  temp$facet <- "All Evidence Configurations"
  merged <-rbind(temp, df)

  # ensure the facet value is a factor
  merged[[col]] <- as.factor(merged[[col]])

  return(merged)
}

plot_df <- CreateAllFacet(median_warming, "evidence")
```


```{r}
# order of the facet factor
facet_order <- c("All Evidence Configurations",
                 "Baseline",
                 "No Process",
                 "No Historical",
                 "No Paleoclimate",
                 "Baseline + Emergent constraints")

# Convert the facet variable to a factor with the desired order
plot_df$facet <- factor(plot_df$facet, levels = facet_order)

temp_plot <- 
  ggplot(data = subset(plot_df,
                       year > 1949,
                       year < 2101)) +
  geom_line(aes(x = year, 
                y = median_warming_wt,
                color = evidence),
            linewidth = 0.75) +
  scale_color_manual(values = c("#003466", "#00a9cf", "#550307", "#EBCC2A", "#F21A00")) +
  geom_ribbon(aes(x = year, 
                  ymin = CI_5,
                  ymax = CI_95,
                  fill = evidence,
                  color = evidence),
              alpha = 0.1,
              linetype = "dashed") +
  scale_fill_manual(values = c("#003466", "#00a9cf", "#550307", "#EBCC2A", "#F21A00")) +
  labs(x = "Year",
       y = expression(paste("Temperature Anomaly (", degree, "C)")),
       title = "A)") +
  theme_light(base_size = 24) +
  theme(legend.position = "none") +
  facet_wrap(~ facet)
temp_plot

ggsave("figures/temp_projections.png",
       plot = temp_plot,
       device = "png",
       width = 17.26,
       height = 11.39, 
       units = "in",
       dpi = "print")
  
```

```{r}

sample_metric <- metric_df %>% 
  group_by(evidence) %>% 
  sample_n(size = 9993, replace = F)

sample_metric$evidence <- recode(
  sample_metric$evidence,
  UL = "Baseline",
  no_process = "No Process",
  no_historical = "No Historical",
  no_paleo = "No Paleoclimate",
  EC_UL = "Baseline + Emergent constraints")

median_metric <- metric_df %>% 
  group_by(evidence) %>% 
  reframe(median_warming_val = quantile(metric_result, probs = 0.5),
          CI_5 = quantile(metric_result, probs = 0.05),
          CI_95 = quantile(metric_result, probs = 0.95))

median_metric$evidence <- recode_factor(
  median_metric$evidence,
  UL = "Baseline",
  no_process = "No Process",
  no_historical = "No Historical",
  no_paleo = "No Paleoclimate",
  EC_UL = "Baseline + Emergent constraints")

median_metric

# Calculate the maximum metric result for each group
max_values <- sample_metric %>%
  group_by(evidence) %>%
  summarise(max_metric_result = max(metric_result))

# Merge the calculated maximum values back into the median_metric data
median_metric <- left_join(median_metric, max_values, by = "evidence")

median_metric
```

```{r}
point_plot <- 
  ggplot() +
    geom_point(data = sample_metric,
         aes(x = evidence,
             y = metric_result,
             fill = evidence,
             color = evidence),
         position = position_jitter(seed = 1, width = 0.2),
         alpha = 0.03) +
  see::geom_violinhalf(data = sample_metric,
              aes(x = evidence,
                  y = metric_result,
                  fill = evidence),
              alpha = 1,
              trim = T) +
  geom_point(data = median_metric,
             aes(x = evidence,
                 y = median_warming_val),
             size = 3,
             color = "gray50") +
  geom_errorbar(data = median_metric,
                aes(x = evidence,
                    ymin = CI_5,
                    ymax = CI_95),
                alpha = 1,
                width = 0.43,
                linewidth = 1,
                color = "grey50") +
  geom_label(data = median_metric,
          aes(x = evidence,
              y = max_metric_result + 0.1,
              label = paste("Median: ", round(median_warming_val, 2), "\n[", round(CI_5, 2), ", ", round(CI_95, 2), "]")),
          size = 5) +
  labs(x = NULL, 
       y = expression(paste("Temperature Anomaly (", degree, "C)")),
       title = "B)") +
  ylim(0, 8) +
  scale_color_manual(values = c("#003466", "#F21A00", "#550307", "#EBCC2A","#00a9cf"),
                     guide = "none") +
  scale_fill_manual(values = c("#003466", "#F21A00", "#550307", "#EBCC2A","#00a9cf"),
                    guide = "none") +
  scale_x_discrete(limit = rev(evidence_order)) +
  theme_light(base_size = 24) +
  theme(axis.text.y = element_blank()) +
  coord_flip()
point_plot

ggsave("figures/temp_distributions.png",
       device = "png",
       width = 10.48,
       height = 11.47, 
       units = "in",
       dpi = "print")

```
____
```{r}
SSP_COLORS <- c("ssp119" = "#00a9cf", "ssp126" = "#003466", "ssp245" = "#f69320",
                "ssp370" = "#df0000", "ssp434" = "#2274ae","ssp460" = "#b0724e",
                "ssp585"= "#980002", "historical" = "#000000", "historical"="#92397a")
```

```{r}
ridge_plot <- 
  ggplot() +
  ggridges::geom_density_ridges(data = sample_metric,
         aes(x = metric_result,
             y = evidence,
             fill = evidence),
         rel_min_height = 0.01,
         stat = "binline",
         bins = 40, 
         scale = 0.95,
         draw_baseline = T) +
  geom_point(data = median_metric,
             aes(x = median_warming_val,
                 y = evidence),
             size = 3,
             color = "black",
             alpha = 0.7,
             shape = 1) +
  
ridge_plot
```

```{r}
ridge_plot <- 
  ggplot() +
  ggridges::geom_density_ridges(data = sample_metric,
         aes(x = metric_result,
             y = evidence,
             fill = evidence,
             point_fill = evidence),
         draw_baseline = T,
         jittered_points = T,
         point_size = 1, 
         point_alpha = 0.05,
         alpha = 0.7,
         stat = "binline",
         bins = 40, 
         scale = 0.95,
         draw_baseline = T) +
  scale_color_manual(values = c("#003466", "#00a9cf", "#550307", "#EBCC2A", "#F21A00"),
                     guide = "none") +
  scale_fill_manual(values = c("#003466", "#00a9cf", "#550307", "#EBCC2A", "#F21A00"),
                    name = "Evidence Line",
                    labels = c("Baseline + Emergent Constraints",
                                 "No Historical",
                                 "No Paleoclimate",
                                 "No Process",
                                 "Baseline"))
ridge_plot
```



# Other notes etc.

Visualization 

#### *This has already been run - do not run again unless you need to save a new file.* 
Bind the rows of the dfs together to make a final large df with all results and save.

```{r}
result_df <- do.call(rbind, result_na_rm)
write.csv(result_df, file = "data/result_df_10k_na_rm.csv")
```




```{r}
y <- data.frame(
  value = rtrunc(100000, spec = "gamma", a = 1.0, b = 8.0, shape = 17.56, rate = 5.29))
```

```{r}
ggplot() +
  geom_density(data = y,
               aes(x = value)) +
  theme_light()
```

